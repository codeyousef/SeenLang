// Comprehensive Lexer Tests for Self-Hosting Compiler
// Tests MUST pass before implementing lexer - TDD approach

import testing.{Test, TestSuite, assert, assertEqual, assertNotEqual, assertThrows}
import lexer.{SeenLexer, Token, TokenType, LexerError}

class LexerTestSuite extends TestSuite {
    fun new() -> LexerTestSuite {
        super("Lexer Tests")
        registerAllTests()
    }
    
    fun registerAllTests() {
        addTest("test_basic_tokens")
        addTest("test_keywords_dynamic_loading")
        addTest("test_string_literals")
        addTest("test_string_interpolation")
        addTest("test_numeric_literals")
        addTest("test_operators")
        addTest("test_word_operators")
        addTest("test_comments")
        addTest("test_nullable_operators")
        addTest("test_position_tracking")
        addTest("test_error_recovery")
        addTest("test_unicode_support")
        addTest("test_multilingual_keywords")
        addTest("test_performance_requirements")
    }
    
    // Test 1: Basic token recognition
    @Test
    fun test_basic_tokens() {
        let source = "fun main() { let x = 42; }"
        let lexer = SeenLexer.new(source, 1, "en")
        let tokens = lexer.tokenize()
        
        assertEqual(tokens.length(), 11)
        assertEqual(tokens[0].getType(), TokenType.KeywordFun)
        assertEqual(tokens[1].getType(), TokenType.Identifier)
        assertEqual(tokens[1].getValue(), "main")
        assertEqual(tokens[2].getType(), TokenType.LeftParen)
        assertEqual(tokens[3].getType(), TokenType.RightParen)
        assertEqual(tokens[4].getType(), TokenType.LeftBrace)
        assertEqual(tokens[5].getType(), TokenType.KeywordLet)
        assertEqual(tokens[6].getType(), TokenType.Identifier)
        assertEqual(tokens[7].getType(), TokenType.Equal)
        assertEqual(tokens[8].getType(), TokenType.IntegerLiteral)
        assertEqual(tokens[8].getValue(), "42")
        assertEqual(tokens[9].getType(), TokenType.Semicolon)
        assertEqual(tokens[10].getType(), TokenType.RightBrace)
    }
    
    // Test 2: Dynamic keyword loading from TOML
    @Test
    fun test_keywords_dynamic_loading() {
        // Test English keywords
        let lexer_en = SeenLexer.new("fun if let var", 1, "en")
        let tokens_en = lexer_en.tokenize()
        
        assertEqual(tokens_en[0].getType(), TokenType.KeywordFun)
        assertEqual(tokens_en[1].getType(), TokenType.KeywordIf)
        assertEqual(tokens_en[2].getType(), TokenType.KeywordLet)
        assertEqual(tokens_en[3].getType(), TokenType.KeywordVar)
        
        // Test Arabic keywords
        let lexer_ar = SeenLexer.new("دالة اذا متغير", 1, "ar")
        let tokens_ar = lexer_ar.tokenize()
        
        assertEqual(tokens_ar[0].getType(), TokenType.KeywordFun)
        assertEqual(tokens_ar[1].getType(), TokenType.KeywordIf)
        assertEqual(tokens_ar[2].getType(), TokenType.KeywordLet)
        
        // Verify no hardcoded keywords
        assertNoHardcodedKeywords(lexer_en)
        assertNoHardcodedKeywords(lexer_ar)
    }
    
    // Test 3: String literals with escapes
    @Test
    fun test_string_literals() {
        let source = """
        "simple string"
        "string with \"quotes\""
        "string with \\n newline"
        "unicode: مرحبا"
        """
        
        let lexer = SeenLexer.new(source, 1, "en")
        let tokens = lexer.tokenize()
        
        let stringTokens = tokens.filter(t -> t.getType() == TokenType.StringLiteral)
        assertEqual(stringTokens.length(), 4)
        assertEqual(stringTokens[0].getValue(), "simple string")
        assertEqual(stringTokens[1].getValue(), "string with \"quotes\"")
        assertEqual(stringTokens[2].getValue(), "string with \n newline")
        assertEqual(stringTokens[3].getValue(), "unicode: مرحبا")
    }
    
    // Test 4: String interpolation
    @Test
    fun test_string_interpolation() {
        let source = """
        "Hello, {name}!"
        "Result: {x + y}"
        "Nested: {getValue("key")}"
        """
        
        let lexer = SeenLexer.new(source, 1, "en")
        let tokens = lexer.tokenize()
        
        // Verify interpolated strings are properly tokenized
        let interpolationTokens = tokens.filter(t -> 
            t.getType() == TokenType.InterpolatedStringStart ||
            t.getType() == TokenType.InterpolatedStringMiddle ||
            t.getType() == TokenType.InterpolatedStringEnd
        )
        
        assert(interpolationTokens.length() > 0)
        // Detailed assertions for interpolation structure
        validateInterpolationStructure(tokens)
    }
    
    // Test 5: Numeric literals
    @Test
    fun test_numeric_literals() {
        let source = "42 3.14 0x1F 0b1010 1_000_000"
        let lexer = SeenLexer.new(source, 1, "en")
        let tokens = lexer.tokenize()
        
        let numbers = tokens.filter(t -> 
            t.getType() == TokenType.IntegerLiteral ||
            t.getType() == TokenType.FloatLiteral
        )
        
        assertEqual(numbers.length(), 5)
        assertEqual(numbers[0].getValue(), "42")
        assertEqual(numbers[1].getValue(), "3.14")
        assertEqual(numbers[2].getValue(), "0x1F")
        assertEqual(numbers[3].getValue(), "0b1010")
        assertEqual(numbers[4].getValue(), "1_000_000")
    }
    
    // Test 6: All operators
    @Test
    fun test_operators() {
        let source = "+ - * / % == != < > <= >= = += -= *="
        let lexer = SeenLexer.new(source, 1, "en")
        let tokens = lexer.tokenize()
        
        let operators = tokens.filter(t -> t.getType().isOperator())
        assertEqual(operators.length(), 13)
        
        // Verify each operator is correctly tokenized
        assertEqual(operators[0].getType(), TokenType.Plus)
        assertEqual(operators[1].getType(), TokenType.Minus)
        assertEqual(operators[2].getType(), TokenType.Multiply)
        assertEqual(operators[3].getType(), TokenType.Divide)
        assertEqual(operators[4].getType(), TokenType.Modulo)
        assertEqual(operators[5].getType(), TokenType.EqualEqual)
        assertEqual(operators[6].getType(), TokenType.NotEqual)
        assertEqual(operators[7].getType(), TokenType.LessThan)
        assertEqual(operators[8].getType(), TokenType.GreaterThan)
        assertEqual(operators[9].getType(), TokenType.LessEqual)
        assertEqual(operators[10].getType(), TokenType.GreaterEqual)
        assertEqual(operators[11].getType(), TokenType.Equal)
        assertEqual(operators[12].getType(), TokenType.PlusEqual)
    }
    
    // Test 7: Word operators (and, or, not)
    @Test
    fun test_word_operators() {
        let source = "x and y or not z"
        let lexer = SeenLexer.new(source, 1, "en")
        let tokens = lexer.tokenize()
        
        assertEqual(tokens[1].getType(), TokenType.LogicalAnd)
        assertEqual(tokens[3].getType(), TokenType.LogicalOr)
        assertEqual(tokens[4].getType(), TokenType.LogicalNot)
        
        // Test Arabic word operators
        let source_ar = "س و ص أو ليس ع"
        let lexer_ar = SeenLexer.new(source_ar, 1, "ar")
        let tokens_ar = lexer_ar.tokenize()
        
        assertEqual(tokens_ar[1].getType(), TokenType.LogicalAnd)
        assertEqual(tokens_ar[3].getType(), TokenType.LogicalOr)
        assertEqual(tokens_ar[4].getType(), TokenType.LogicalNot)
    }
    
    // Test 8: Comments
    @Test
    fun test_comments() {
        let source = """
        // Single line comment
        /* Multi-line
           comment */
        fun main() // Inline comment
        """
        
        let lexer = SeenLexer.new(source, 1, "en")
        let tokens = lexer.tokenize()
        
        // Comments should not appear in token stream
        let commentTokens = tokens.filter(t -> t.getType() == TokenType.Comment)
        assertEqual(commentTokens.length(), 0)
        
        // But code should still be tokenized correctly
        let identifiers = tokens.filter(t -> t.getType() == TokenType.Identifier)
        assertEqual(identifiers[0].getValue(), "main")
    }
    
    // Test 9: Nullable operators
    @Test
    fun test_nullable_operators() {
        let source = "user?.name ?: \"unknown\""
        let lexer = SeenLexer.new(source, 1, "en")
        let tokens = lexer.tokenize()
        
        assertEqual(tokens[1].getType(), TokenType.SafeNavigation)
        assertEqual(tokens[3].getType(), TokenType.Elvis)
    }
    
    // Test 10: Position tracking
    @Test
    fun test_position_tracking() {
        let source = """line1
        line2 token
        line3"""
        
        let lexer = SeenLexer.new(source, 1, "en")
        let tokens = lexer.tokenize()
        
        let identifiers = tokens.filter(t -> t.getType() == TokenType.Identifier)
        
        // line1 should be at line 1, column 1
        assertEqual(identifiers[0].getLine(), 1)
        assertEqual(identifiers[0].getColumn(), 1)
        
        // line2 should be at line 2, column 9
        assertEqual(identifiers[1].getLine(), 2)
        assertEqual(identifiers[1].getColumn(), 9)
        
        // token should be at line 2, column 15
        assertEqual(identifiers[2].getLine(), 2)
        assertEqual(identifiers[2].getColumn(), 15)
        
        // line3 should be at line 3, column 9
        assertEqual(identifiers[3].getLine(), 3)
        assertEqual(identifiers[3].getColumn(), 9)
    }
    
    // Test 11: Error recovery
    @Test
    fun test_error_recovery() {
        let source = "let x = @invalid 42" // Invalid character @
        
        let lexer = SeenLexer.new(source, 1, "en")
        
        // Should throw LexerError for invalid character
        assertThrows(() -> lexer.tokenize(), LexerError)
        
        // Test recovery mode
        lexer.setErrorRecoveryMode(true)
        let tokens = lexer.tokenize()
        
        // Should continue after error and tokenize remaining valid tokens
        let validTokens = tokens.filter(t -> t.getType() != TokenType.Error)
        assert(validTokens.length() > 0)
    }
    
    // Test 12: Unicode support
    @Test
    fun test_unicode_support() {
        let source = "let مرحبا = \"Hello\"; let 你好 = \"世界\""
        let lexer = SeenLexer.new(source, 1, "en")
        let tokens = lexer.tokenize()
        
        let identifiers = tokens.filter(t -> t.getType() == TokenType.Identifier)
        assertEqual(identifiers[0].getValue(), "مرحبا")
        assertEqual(identifiers[1].getValue(), "你好")
    }
    
    // Test 13: Multilingual keywords
    @Test
    fun test_multilingual_keywords() {
        // Test all supported languages load correctly
        let languages = ["en", "ar", "es", "fr", "de", "zh", "ja", "pt", "ru", "hi"]
        
        for language in languages {
            let lexer = SeenLexer.new("", 1, language)
            assert(lexer.getKeywordCount() > 0)
            assert(lexer.hasKeyword("fun") || lexer.hasTranslatedKeyword("fun"))
        }
    }
    
    // Test 14: Performance requirements
    @Test
    fun test_performance_requirements() {
        // Generate large source code (100K tokens)
        let largeSource = generateLargeSource(100000)
        
        let lexer = SeenLexer.new(largeSource, 1, "en")
        
        let startTime = getCurrentTime()
        let tokens = lexer.tokenize()
        let endTime = getCurrentTime()
        
        let durationMs = endTime - startTime
        let tokensPerSecond = (tokens.length() * 1000) / durationMs
        
        // Must achieve at least 14M tokens/sec
        assert(tokensPerSecond >= 14_000_000)
        
        // Memory usage should be reasonable
        let memoryUsedMB = lexer.getMemoryUsage() / 1024 / 1024
        assert(memoryUsedMB < 100) // Less than 100MB for 100K tokens
    }
    
    // Helper methods
    fun assertNoHardcodedKeywords(lexer: SeenLexer) {
        // Verify lexer loads keywords from TOML files only
        assert(lexer.isKeywordSourceDynamic())
        assert(lexer.getKeywordSources().all(source -> source.endsWith(".toml")))
    }
    
    fun validateInterpolationStructure(tokens: Array<Token>) {
        // Detailed validation of interpolation token structure
        // Implementation depends on exact interpolation design
        assert(tokens.length() > 0)
    }
    
    fun generateLargeSource(tokenCount: Int) -> String {
        let builder = StringBuilder()
        for i in 0..tokenCount/10 {
            builder.append("fun function{i}() { let x{i} = {i}; }\n")
        }
        return builder.toString()
    }
    
    fun getCurrentTime() -> Int {
        // System time in milliseconds
        return 0 // Implementation depends on runtime
    }
}

// Export test suite for test runner
export fun createLexerTests() -> TestSuite {
    return LexerTestSuite.new()
}
// Self-hosted Seen Lexer
// Ported from Rust implementation with performance optimizations
// Target: >25M tokens/sec (allowing some overhead for self-hosting)

import std.io.File;
import std.collections.Vec;
import std.core.Result;
import std.core.Option;
import std.string.String;

import "token.seen";
import "language_config.seen";

// Main lexer structure - handles tokenization of Seen source code
struct SeenLexer {
    input: str,
    position: i32,
    current_char: Option<char>,
    line: i32,
    column: i32,
    language_config: LanguageConfig,
}

impl SeenLexer {
    // Create a new lexer instance
    fun new() -> Self {
        Self {
            input: "",
            position: 0,
            current_char: None,
            line: 1,
            column: 1,
            language_config: LanguageConfig::english(),
        }
    }
    
    // Create lexer with specific language configuration
    fun with_language(language: str) -> Self {
        let config = match language {
            "en" => LanguageConfig::english(),
            "ar" => LanguageConfig::arabic(),
            _ => LanguageConfig::english(),
        };
        
        Self {
            input: "",
            position: 0,
            current_char: None,
            line: 1,
            column: 1,
            language_config: config,
        }
    }
    
    // Tokenize source code from string
    fun tokenize(mut self, input: str) -> Result<Vec<Token>, str> {
        self.input = input;
        self.position = 0;
        self.line = 1;
        self.column = 1;
        self.current_char = if input.len() > 0 { Some(input.char_at(0)) } else { None };
        
        let mut tokens = Vec::new();
        
        while self.current_char.is_some() {
            let token = self.next_token()?;
            
            if token.token_type != TokenType::EOF {
                tokens.push(token);
            } else {
                break;
            }
        }
        
        // Add EOF token
        tokens.push(Token {
            token_type: TokenType::EOF,
            lexeme: "",
            line: self.line,
            column: self.column,
        });
        
        Ok(tokens)
    }
    
    // Tokenize entire project (multiple files)
    fun tokenize_project(self, project_path: str) -> Result<Vec<Token>, str> {
        let mut all_tokens = Vec::new();
        
        // Find all .seen files in the project
        let seen_files = self.find_seen_files(project_path)?;
        
        for file_path in seen_files {
            let file_content = File::read_to_string(file_path)?;
            let mut file_lexer = Self::new();
            let file_tokens = file_lexer.tokenize(file_content)?;
            all_tokens.extend(file_tokens);
        }
        
        Ok(all_tokens)
    }
    
    // Get next token from input stream
    fun next_token(mut self) -> Result<Token, str> {
        self.skip_whitespace_and_comments();
        
        let start_line = self.line;
        let start_column = self.column;
        
        match self.current_char {
            None => Ok(Token {
                token_type: TokenType::EOF,
                lexeme: "",
                line: start_line,
                column: start_column,
            }),
            
            Some(ch) => {
                match ch {
                    // Single character tokens
                    '(' => self.make_single_char_token(TokenType::LeftParen),
                    ')' => self.make_single_char_token(TokenType::RightParen),
                    '{' => self.make_single_char_token(TokenType::LeftBrace),
                    '}' => self.make_single_char_token(TokenType::RightBrace),
                    '[' => self.make_single_char_token(TokenType::LeftBracket),
                    ']' => self.make_single_char_token(TokenType::RightBracket),
                    ',' => self.make_single_char_token(TokenType::Comma),
                    ';' => self.make_single_char_token(TokenType::Semicolon),
                    
                    // Potentially multi-character tokens
                    '+' => self.scan_plus_or_compound(),
                    '-' => self.scan_minus_or_compound(),
                    '*' => self.scan_multiply_or_compound(),
                    '/' => self.scan_divide_or_compound(),
                    '%' => self.scan_modulo_or_compound(),
                    '=' => self.scan_equals_or_compound(),
                    '!' => self.scan_not_or_compound(),
                    '<' => self.scan_less_or_compound(),
                    '>' => self.scan_greater_or_compound(),
                    '&' => self.scan_and_or_compound(),
                    '|' => self.scan_or_or_compound(),
                    '^' => self.scan_xor_or_compound(),
                    ':' => self.scan_colon_or_compound(),
                    '.' => self.scan_dot_or_compound(),
                    '?' => self.make_single_char_token(TokenType::Question),
                    
                    // String literals
                    '"' => self.scan_string_literal(),
                    '\'' => self.scan_char_literal(),
                    
                    // Numeric literals
                    '0'..='9' => self.scan_number_literal(),
                    
                    // Identifiers and keywords (including Unicode for Arabic)
                    _ if ch.is_alphabetic() || ch == '_' || self.is_arabic_letter(ch) => {
                        self.scan_identifier_or_keyword()
                    },
                    
                    // Unexpected character
                    _ => Err(format!("Unexpected character '{}' at line {}, column {}", ch, start_line, start_column)),
                }
            }
        }
    }
    
    // Skip whitespace and comments
    fun skip_whitespace_and_comments(mut self) {
        while let Some(ch) = self.current_char {
            match ch {
                // Whitespace
                ' ' | '\t' | '\r' => self.advance(),
                
                // Newlines (track line numbers)
                '\n' => {
                    self.line += 1;
                    self.column = 1;
                    self.advance();
                },
                
                // Line comments
                '/' if self.peek() == Some('/') => {
                    self.skip_line_comment();
                },
                
                // Block comments  
                '/' if self.peek() == Some('*') => {
                    self.skip_block_comment();
                },
                
                // Not whitespace or comment
                _ => break,
            }
        }
    }
    
    // Skip line comment (// comment)
    fun skip_line_comment(mut self) {
        // Skip //
        self.advance();
        self.advance();
        
        // Skip until end of line
        while let Some(ch) = self.current_char {
            if ch == '\n' {
                break;
            }
            self.advance();
        }
    }
    
    // Skip block comment (/* comment */)
    fun skip_block_comment(mut self) {
        // Skip /*
        self.advance();
        self.advance();
        
        while let Some(ch) = self.current_char {
            if ch == '*' && self.peek() == Some('/') {
                // Skip */
                self.advance();
                self.advance();
                break;
            }
            
            if ch == '\n' {
                self.line += 1;
                self.column = 1;
            }
            
            self.advance();
        }
    }
    
    // Advance to next character
    fun advance(mut self) {
        self.position += 1;
        self.column += 1;
        
        if self.position >= self.input.len() {
            self.current_char = None;
        } else {
            self.current_char = Some(self.input.char_at(self.position));
        }
    }
    
    // Peek at next character without advancing
    fun peek(self) -> Option<char> {
        let next_pos = self.position + 1;
        if next_pos >= self.input.len() {
            None
        } else {
            Some(self.input.char_at(next_pos))
        }
    }
    
    // Make single character token and advance
    fun make_single_char_token(mut self, token_type: TokenType) -> Result<Token, str> {
        let lexeme = self.current_char.unwrap().to_string();
        let line = self.line;
        let column = self.column;
        
        self.advance();
        
        Ok(Token {
            token_type,
            lexeme,
            line,
            column,
        })
    }
    
    // Scan identifier or keyword
    fun scan_identifier_or_keyword(mut self) -> Result<Token, str> {
        let start_line = self.line;
        let start_column = self.column;
        let start_pos = self.position;
        
        // Collect identifier characters
        while let Some(ch) = self.current_char {
            if ch.is_alphanumeric() || ch == '_' || self.is_arabic_letter(ch) {
                self.advance();
            } else {
                break;
            }
        }
        
        let lexeme = self.input.substring(start_pos, self.position);
        
        // Check if it's a keyword in the current language
        let token_type = self.language_config.get_keyword_type(lexeme)
            .unwrap_or(TokenType::Identifier);
        
        Ok(Token {
            token_type,
            lexeme,
            line: start_line,
            column: start_column,
        })
    }
    
    // Scan string literal
    fun scan_string_literal(mut self) -> Result<Token, str> {
        let start_line = self.line;
        let start_column = self.column;
        let start_pos = self.position;
        
        // Skip opening quote
        self.advance();
        
        while let Some(ch) = self.current_char {
            match ch {
                '"' => {
                    // Skip closing quote
                    self.advance();
                    break;
                },
                '\\' => {
                    // Handle escape sequences
                    self.advance();
                    if self.current_char.is_some() {
                        self.advance();
                    }
                },
                '\n' => {
                    self.line += 1;
                    self.column = 1;
                    self.advance();
                },
                _ => self.advance(),
            }
        }
        
        let lexeme = self.input.substring(start_pos, self.position);
        
        Ok(Token {
            token_type: TokenType::StringLiteral,
            lexeme,
            line: start_line,
            column: start_column,
        })
    }
    
    // Scan numeric literal (integers and floats)
    fun scan_number_literal(mut self) -> Result<Token, str> {
        let start_line = self.line;
        let start_column = self.column;
        let start_pos = self.position;
        
        let mut is_float = false;
        
        // Scan digits
        while let Some(ch) = self.current_char {
            if ch.is_digit(10) {
                self.advance();
            } else if ch == '.' && !is_float && self.peek().map(|c| c.is_digit(10)).unwrap_or(false) {
                is_float = true;
                self.advance(); // consume '.'
            } else {
                break;
            }
        }
        
        let lexeme = self.input.substring(start_pos, self.position);
        let token_type = if is_float { 
            TokenType::FloatLiteral 
        } else { 
            TokenType::IntegerLiteral 
        };
        
        Ok(Token {
            token_type,
            lexeme,
            line: start_line,
            column: start_column,
        })
    }
    
    // Check if character is Arabic letter (for Arabic language support)
    fun is_arabic_letter(self, ch: char) -> bool {
        // Arabic Unicode range: U+0600 to U+06FF
        let code = ch as u32;
        code >= 0x0600 && code <= 0x06FF
    }
    
    // Find all .seen files in project directory
    fun find_seen_files(self, project_path: str) -> Result<Vec<str>, str> {
        // Implementation to recursively find .seen files
        // This would use the file system API to traverse directories
        
        let mut seen_files = Vec::new();
        // Add implementation here
        Ok(seen_files)
    }
    
    // Compound operator scanning methods (simplified for now)
    fun scan_plus_or_compound(mut self) -> Result<Token, str> {
        if self.peek() == Some('=') {
            self.advance();
            self.advance();
            Ok(Token {
                token_type: TokenType::PlusEqual,
                lexeme: "+=",
                line: self.line,
                column: self.column - 2,
            })
        } else {
            self.make_single_char_token(TokenType::Plus)
        }
    }
    
    fun scan_minus_or_compound(mut self) -> Result<Token, str> {
        if self.peek() == Some('=') {
            self.advance();
            self.advance();
            Ok(Token {
                token_type: TokenType::MinusEqual,
                lexeme: "-=",
                line: self.line,
                column: self.column - 2,
            })
        } else {
            self.make_single_char_token(TokenType::Minus)
        }
    }
    
    // Additional compound operator methods would be implemented here...
    // For brevity, including only a few examples
    
    fun scan_multiply_or_compound(mut self) -> Result<Token, str> { self.make_single_char_token(TokenType::Star) }
    fun scan_divide_or_compound(mut self) -> Result<Token, str> { self.make_single_char_token(TokenType::Slash) }
    fun scan_modulo_or_compound(mut self) -> Result<Token, str> { self.make_single_char_token(TokenType::Percent) }
    fun scan_equals_or_compound(mut self) -> Result<Token, str> { self.make_single_char_token(TokenType::Equal) }
    fun scan_not_or_compound(mut self) -> Result<Token, str> { self.make_single_char_token(TokenType::Bang) }
    fun scan_less_or_compound(mut self) -> Result<Token, str> { self.make_single_char_token(TokenType::Less) }
    fun scan_greater_or_compound(mut self) -> Result<Token, str> { self.make_single_char_token(TokenType::Greater) }
    fun scan_and_or_compound(mut self) -> Result<Token, str> { self.make_single_char_token(TokenType::Ampersand) }
    fun scan_or_or_compound(mut self) -> Result<Token, str> { self.make_single_char_token(TokenType::Pipe) }
    fun scan_xor_or_compound(mut self) -> Result<Token, str> { self.make_single_char_token(TokenType::Caret) }
    fun scan_colon_or_compound(mut self) -> Result<Token, str> { self.make_single_char_token(TokenType::Colon) }
    fun scan_dot_or_compound(mut self) -> Result<Token, str> { self.make_single_char_token(TokenType::Dot) }
    fun scan_char_literal(mut self) -> Result<Token, str> { self.make_single_char_token(TokenType::CharLiteral) }
}